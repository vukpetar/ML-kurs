{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "\n",
    "U pokaznom primjeru implementirali smo dvoslojnu neuralnu mrežu i koristili smo CIFAR-10 korpus podataka. Implementacija je bila jednostavna, ali isto tako i nepraktična jer su loss i gradijent računani u jednoj monolitnoj funkciji. Ovaj pristup je prihvatljiv za jednostavnu dvoslojnu mrežu, ali se ne može održati za veće modele. Želimo da napravimo mreže čiji su djelovi nezavisni tako da ih po potrebi možemo poređati u arhitekturu po želji.\n",
    "\n",
    "U ovoj vježbi napravićemo jednu takvu mrežu. Za svaki sloj imaćemo `forward` i `backward` funkciju. `forward` funkcija će imati za cilj da za primljene ulazne podatke, težine i ostale parametre izračuna kako izlaz tako i `cache` koji će čuvati vrijednosti za prolaz unazad kao što se može vidjeti u kodu:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "    \"\"\" Kao ulaz prima podatke x i parametre w \"\"\"\n",
    "    # radimo neke proračune ...\n",
    "    z = # ... dobijamo neku vrijednost\n",
    "    # ponovo radimo proračune ...\n",
    "    out = # dobijamo konačnu vrijednost\n",
    "\n",
    "    cache = (x, w, z, out) # Vrijednosti koje su nam potrebne da izračunamo gradijente\n",
    "\n",
    "    return out, cache\n",
    "```\n",
    "\n",
    "Prolaz unazad će uzeti gradijente \"odozgo\" i `cache` i izračunaće gradijente u odnosu na ulaz i težine:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Kao ulaz prima dout (izvod loss-a u odnosu na izlaz) i cache, i\n",
    "    računa izvod u odnosu na ulaz.\n",
    "    \"\"\"\n",
    "    # cache smo gore \"spakovali\" kao tuple pa ga sada otvaramo\n",
    "    x, w, z, out = cache\n",
    "\n",
    "    # Koristimo vrijednosti iz cache-a da dobijemo izvode\n",
    "    dx = # Izvod loss-a po x\n",
    "    dw = # Izvod loss-a po w\n",
    "\n",
    "    return dx, dw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na početku svakog zadatka moramo izvršiti import paketa koji su nam potrebni\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from funkcije.data_utils import get_CIFAR10_data\n",
    "from funkcije.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from funkcije.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # postavljamo podrazumijevanu veličinu figure\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# pogledati http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" vraća relativnu grešku \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Učitavanje obrađenih CIFAR10 podataka.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afina sloj: foward\n",
    "Otvorite file `funkcije/layers.py` i implementirajte `affine_forward` funkciju.\n",
    "\n",
    "Kada završite testirajte se:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ovaj dio koda služi za testiranje funkcije affine_forward\n",
    "from funkcije.layers import *\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num = input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num = weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num = output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Uporedite vaše rezultate sa našim. Greška bi trebalo da je oko e-9 ili čak manja.\n",
    "print('Testiranje affine_forward funkcije:')\n",
    "print('razlika: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afina sloj: backward\n",
    "Sada implementirajte `affine_backward` funkciju i uporedite dobijene rezultate sa numeričkim izvodom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ovaj dio koda služi za testiranje funkcije affine_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# Greška bi trebalo da je e-10 ili manja\n",
    "print('Testiranje affine_backward funkcije:')\n",
    "print('dx greška: ', rel_error(dx_num, dx))\n",
    "print('dw greška: ', rel_error(dw_num, dw))\n",
    "print('db greška: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU aktivacija: forward\n",
    "\n",
    "Implementirajte prolaz unaprijed za ReLU aktivacionu funkciju u `relu_forward` i testirajte svoju implementaciju koristeći sledeći kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ovaj dio koda služi za testiranje funkcije relu_forward\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num = 12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Uporedite rezultate sa našim. Greška bi trebalo da je reda e-8\n",
    "print('Testiranje relu_forward funkcije:')\n",
    "print('razlika: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU aktivacija: backward\n",
    "Sada implementirajte prolazak unazad za ReLU aktivacionu funkciju u `relu_backward` i testirajte svoju implementaciju koristeći sledeći kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# Greška bi trebalo da je reda e-12\n",
    "print('Testiranje relu_backward funkcije:')\n",
    "print('dx greška: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sendvič\" slojevi\n",
    "Postoje šabloni slojeva koji se često koriste u neuralnim mrežama. Na primjer, posle afina slojeva često dolazi ReLU nelinearnost. Kako bi ovu učestanost iskoristili, definisaćemo par slojeva u `funkcije/layer_utils.py`.\n",
    "\n",
    "Za sad bacite pogleda na `affine_relu_forward` i `affine_relu_backward` funkcije, i pokrenite sledeći kod kako biste u prolazu unazad provjerili gradijente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funkcije.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Greška bi trebalo da je oko e-10 ili manja\n",
    "print('Testiranje affine_relu_forward i affine_relu_backward:')\n",
    "print('dx greška: ', rel_error(dx_num, dx))\n",
    "print('dw greška: ', rel_error(dw_num, dw))\n",
    "print('db greška: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss slojevi: Softmax i SVM\n",
    "\n",
    "Ove funkcije smo za vas implementirali u pokaznom primjeru pa vam ih dajemo i ovdje. Pogledajte kako funkcionišu (ukoliko niste sigurni) gledanjem `funkcije/layers.py`.\n",
    "\n",
    "Pokrenite kod kako biste se uvjerili da sve radi kako treba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Testirajte svm_loss funkciju. Loss bi trebalo da je oko 9, a dx greška reda e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Testirajte softmax_loss funkciju. Loss bi trebalo da je oko 2.3, a dx greška reda e-8\n",
    "print('\\Testiranje softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx greška: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dvoslojna mreža\n",
    "\n",
    "U pokaznom primjeru implementirali smo dvoslojnu mrežu u jednoj monolitnoj klasi. Sada vi za zadatak imate to da učinite korištenjem funkcija koje ste napravili u ovom zadatku.\n",
    "\n",
    "Otvorite `funkcije/fc_net.py` i završite implementaciju `TwoLayerNet` klase. Ova klasa će služiti kao model za ostale mreže koje ćete implementirati u ovom zadatku. Kod koji slijedi omogućava vam testiranje klase koju ste implementirali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funkcije.fc_net import *\n",
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim = D, hidden_dim = H, num_classes = C, weight_scale = std)\n",
    "\n",
    "print('Testiranje inicijalizacije ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'Težine prvog sloja ne izgledaju dobro'\n",
    "assert np.all(b1 == 0), 'Biasi prvog sloja ne izgledaju dobro'\n",
    "assert W2_std < std / 10, 'Težine drugog sloja ne izgledaju dobro'\n",
    "assert np.all(b2 == 0), 'Biasi drugog sloja ne izgledaju dobro'\n",
    "\n",
    "print('Testiranje test-time prolaza unaprijed ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num = D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num = H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num = H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num = C)\n",
    "X = np.linspace(-5.5, 4.5, num = N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problemi sa test-time prolazom unaprijed'\n",
    "\n",
    "print('Testiranje trening loss-a (bez regularizacije)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problemi sa training-time loss-om'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problemi sa regularizacijom'\n",
    "\n",
    "# Greške bi trebalo da su oko e-7 ili manje\n",
    "for reg in [0.0, 0.7]:\n",
    "    print('Provjeravanje izvoda sa regularizacijom = ', reg)\n",
    "    model.reg = reg\n",
    "    loss, grads = model.loss(X, y)\n",
    "\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "        print('%s relativna greška: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "\n",
    "U pokaznom primjeru je logika treniranja implementirana unutar modela. Praktičnije rješenje iziskuje da se ove dvije stvari razdvoje u različite klase.\n",
    "\n",
    "Otvorite `funkcije/solver.py`. Iskoristite `Solver` instancu da istrenirate `TwoLayerNet` kako biste dobili model koji ima najmanje `50%` tačnost na validacionom setu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# URADITI: Iskoristite Solver instancu da istrenirate TwoLayerNet kako biste    #\n",
    "# dobili model koji ima najmanje 50% tačnost na validacionom setu.           #\n",
    "##############################################################################\n",
    "pass\n",
    "##############################################################################\n",
    "#                             KRAJ VAŠEG KODA                                #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokrenite ovaj kod kako biste vizuelizovali trening loss i trening / validacija preciznost\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Trening loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteracija')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Preciznost')\n",
    "plt.plot(solver.train_acc_history, '-o', label = 'train')\n",
    "plt.plot(solver.val_acc_history, '-o', label = 'val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoha')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Višeslojna mreža\n",
    "\n",
    "Sledeći zadatak jeste da napravimo potpuno povezanu mrežu sa proizvoljnim brojem skrivenih slojeva.\n",
    "\n",
    "Pročitajte sve iz `FullyConnectedNet` klase u `funkcije/fc_net.py`.\n",
    "\n",
    "Implementirajte inicijalizaciju, prolaz unaprijed i prolaz unazad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicijalni loss i provjera gradijenta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provjerite inicijalne vrijednosti mreže sa i bez regularizacije. Za provjeru gradijenata treba očekivati greške reda 1e-7 ili manje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "    print('Provjera sa regularizacijom = ', reg)\n",
    "    model = FullyConnectedNet([H1, H2], input_dim = D, num_classes = C,\n",
    "                            reg = reg, weight_scale = 5e-2, dtype = np.float64)\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print('Inicijalni loss: ', loss)\n",
    "  \n",
    "  # Većina grešaka bi trebalo da je reda e-7 ili manje.   \n",
    "  # NAPOMENA: Za W2 greška može biti reda e-5\n",
    "  # za provjeru kada je reg = 0.0\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose = False, h = 1e-5)\n",
    "        print('%s relativna greška: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za provjeru pokušajte da overfitujete mali korpus podataka od 50 slika. Prvo pokušavamo sa troslojnom mrežom u kojoj skriveni slojevi imaju po 100 neurona. Pokušajte da mijenjate learning_rate i weight_scale kako biste dobili 100% preciznost na trening setu nakon najviše 20 epoha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-4\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale = weight_scale, dtype = np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every = 10, num_epochs = 20, batch_size = 25,\n",
    "                update_rule = 'sgd',\n",
    "                optim_config = {\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Vrijednosti trening loss-a')\n",
    "plt.xlabel('Iteracija')\n",
    "plt.ylabel('Trening loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada pokušajte isto sa petoslojnom mrežom u kojoj svaki sloj ima 100 neurona. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 2e-3\n",
    "weight_scale = 1e-5\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale = weight_scale, dtype = np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every = 10, num_epochs = 20, batch_size = 25,\n",
    "                update_rule = 'sgd',\n",
    "                optim_config = {\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Vrijednosti trening loss-a')\n",
    "plt.xlabel('Iteracija')\n",
    "plt.ylabel('Trening loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pravila za ažuriranje parametara\n",
    "Za sada smo koristili samo `vanila` SGD. Kao što je i na predavanju objašnjeno, postoje mnogo bolji načini optimizacije i neke ćemo implementirati. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Otvorite `funkcije/optim.py` i implementirajte the SGD+momentum pravilo ažuriranja u funkciji `sgd_momentum` i pokrenite kod koji slijedi kako biste provjerili rezultate. Trebalo bi da je greška manja od e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print('next_w greška: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity greška: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kada ste završili sa prethodnim pokrenite sledeći kod kako biste istrenirali šestoslojnu mrežu sa SGD i sa SGD+momentum. Očekujemo da SGD+momentum brže iskonvergira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "    print('radimo sa ', update_rule)\n",
    "    model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale = 5e-2)\n",
    "\n",
    "    solver = Solver(model, small_data,\n",
    "                  num_epochs = 5, batch_size = 100,\n",
    "                  update_rule = update_rule,\n",
    "                  optim_config = {\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose = True)\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "    print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Trening loss')\n",
    "plt.xlabel('Iteracija')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Trening preciznost')\n",
    "plt.xlabel('Epoha')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validacija preciznost')\n",
    "plt.xlabel('Epoha')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(solver.loss_history, 'o', label = update_rule)\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(solver.train_acc_history, '-o', label = update_rule)\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(solver.val_acc_history, '-o', label = update_rule)\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    plt.subplot(3, 1, i)\n",
    "    plt.legend(loc = 'upper center', ncol = 4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp i Adam\n",
    "RMSProp [1] i Adam [2] su pravila ažuriranja parametara koje smo pomenuli na časovima, a koja su detaljnije objašnjena u priloženim radovima.\n",
    "\n",
    "U fajlu `funkcije/optim.py`, implementirajte RMSProp u `rmsprop` funkciji i implementirajte Adam u `adam` funkciji, te provjerite vašu implementaciju u testu koji slijedi. \n",
    "\n",
    "**NAPOMENA:** Za zadatak treba implementirati kompletno Adamovo pravilo (sa mehanizmom za korekciju biasa), a ne onu verziju koja je prikazana na času. Detaljnije u radu.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funkcije.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# Očekujemo grešku manju od e-7\n",
    "print('next_w greška: ', rel_error(expected_next_w, next_w))\n",
    "print('cache greška: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funkcije.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# Očekujemo grešku manju od e-7\n",
    "print('next_w greška: ', rel_error(expected_next_w, next_w))\n",
    "print('v greška: ', rel_error(expected_v, config['v']))\n",
    "print('m greška: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nakon uspješne implementacije ova dva pravila, istrenirajte sledeću mrežu kako biste uočili razlike:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "    print('running with ', update_rule)\n",
    "    model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale = 5e-2)\n",
    "\n",
    "    solver = Solver(model, small_data,\n",
    "                  num_epochs = 5, batch_size = 100,\n",
    "                  update_rule = update_rule,\n",
    "                  optim_config = {\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose = True)\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "    print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Trening loss')\n",
    "plt.xlabel('Iteracija')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Trening preciznost')\n",
    "plt.xlabel('Epoha')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validacija preciznost')\n",
    "plt.xlabel('Epoha')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(solver.loss_history, 'o', label = update_rule)\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(solver.train_acc_history, '-o', label = update_rule)\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(solver.val_acc_history, '-o', label = update_rule)\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    plt.subplot(3, 1, i)\n",
    "    plt.legend(loc = 'upper center', ncol = 4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oprobajte se!\n",
    "\n",
    "Pokušajte da napravite najbolji mogući model na korpusu podataka CIFAR-10 i sačuvajte ga u `best_model`.\n",
    "\n",
    "Takođe, prije pokretanja ovog koda možete sačekati da riješite `BatchNormalization.ipynb` i `Dropout.ipynb` jer značajno pomažu mreži da ostvari bolje rezultate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# URADITI: Napravite najbolji mogući model i sačuvajte ga u best_model         #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              KRAJ VAŠEG KODA                                 #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testirajte model!\n",
    "Pokrenite vaš model na validacionom i test setu. Očekujemo da ostvarite do 55% preciznosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis = 1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis = 1)\n",
    "print('Preciznost na validacionom setu: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Preciznost na test setu: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
