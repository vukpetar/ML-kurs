{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from funkcije.fc_net import *\n",
    "from funkcije.data_utils import get_CIFAR10_data\n",
    "from funkcije.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from funkcije.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" vraća relativnu grešku \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def print_mean_std(x,axis=0):\n",
    "    print('  srednja vrijednost: ', x.mean(axis=axis))\n",
    "    print('  standardna devijacija:  ', x.std(axis=axis))\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Učitati CIFAR-10 podatke\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "    print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalizacija: prolaz unaprijed\n",
    "U fajlu `funkcije/layers.py`, implementirati batch normalizacija prolaz unaprijed u funkciji `batchnorm_forward`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulacija prolaza unaprijed za dvoslojnu mrežu\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Prije batch normalizacije:')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "# Srednje vrijednosti će biti približno nuli, a standardne devijacije jedinici\n",
    "print('Posle batch normalizacije (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "# Sada srednje vrijednosti treba da budu blizu beta, a standardne devijacije blizu gamma\n",
    "print('Posle batch normalizacije (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Prije batch normalizacije:')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "# Srednje vrijednosti će biti približno nuli, a standardne devijacije jedinici\n",
    "print('Posle batch normalizacije (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "# Sada srednje vrijednosti treba da budu blizu beta, a standardne devijacije blizu gamma\n",
    "print('Posle batch normalizacije (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "\n",
    "for t in range(50):\n",
    "    X = np.random.randn(N, D1)\n",
    "    a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "    batchnorm_forward(a, gamma, beta, bn_param)\n",
    "    \n",
    "\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Srednje vrijednosti će biti približno nuli, a standardne devijacije jedinici, ali će biti pod većim šumom\n",
    "print('Posle batch normalizacije (test-time):')\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalizacija: prolaz unazad\n",
    "Sada implementirajte prolaz unazad u funkciji `batchnorm_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, a, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "#Treba očekivati da relativne greške budu između 1e-13 i 1e-8\n",
    "print('dx greška: ', rel_error(dx_num, dx))\n",
    "print('dgamma greška: ', rel_error(da_num, dgamma))\n",
    "print('dbeta greška: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, a, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "#Treba očekivati da relativne greške budu između 1e-13 i 1e-8\n",
    "print('dx greška: ', rel_error(dx_num, dx))\n",
    "print('dgamma greška: ', rel_error(da_num, dgamma))\n",
    "print('dbeta greška: ', rel_error(db_num, dbeta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalizacija: alternativni prolaz unazad\n",
    "Druga strategija za prolaz unazad je napisati izvode na papiru. Na primjer, izvede se jednostavna formula za sigmoid funkciju prolaza unazad pojednostavljujući gradijente na papiru.\n",
    "\n",
    "Iznenađujuće, ispostavlja se da se na sličan način može odraditi ppojednostavljivanje za prolaz unazad batch normalizacije.  \n",
    "Ulazi $X=\\begin{bmatrix}x_1\\\\x_2\\\\...\\\\x_N\\end{bmatrix}$, \n",
    "prvo računamo srednju vrijednost $\\mu=\\frac{1}{N}\\sum_{k=1}^N x_k$ i varijansu $v=\\frac{1}{N}\\sum_{k=1}^N (x_k-\\mu)^2.$    \n",
    "Sa $\\mu$ i $v$ izračunatim, možemo izračunati standardnu devijaciju $\\sigma=\\sqrt{v+\\epsilon}$ i normalizovane podatke $Y$ sa $y_i=\\frac{x_i-\\mu}{\\sigma}.$\n",
    "\n",
    "\n",
    "Srž problema je dobiti $\\frac{\\partial L}{\\partial X}$ za upstream gradijente $\\frac{\\partial L}{\\partial Y}.$ Može biti izazovno za $X$ i $Y$ - dobar je pokušaj razmišljati o $x_i$ i $y_i$ prvo.\n",
    "\n",
    "Treba izvesti $\\frac{\\partial L}{\\partial x_i}$, oslanjajući se na Chain Rule da se prvo izračunaju među izvodi $\\frac{\\partial \\mu}{\\partial x_i}, \\frac{\\partial v}{\\partial x_i}, \\frac{\\partial \\sigma}{\\partial x_i},$ onda sastaviti ove djeliće da se izračuna $\\frac{\\partial y_i}{\\partial x_i}$. Treba težiti da svako od među koraka bude što je moguće jednostavniji. \n",
    "\n",
    "Nakon toga, implementirajte jednostavnu batch normalizaciju prolaz unazad u funkciji `batchnorm_backward_alt`. Ove dvije implementacije neće dati identične rezultate, ali bi ovaj alternativni trebao da bude brži."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print('dx razlika: ', rel_error(dx1, dx2))\n",
    "print('dgamma razlika: ', rel_error(dgamma1, dgamma2))\n",
    "print('dbeta razlika: ', rel_error(dbeta1, dbeta2))\n",
    "print('speedup: %.2fx' % ((t2 - t1) / (t3 - t2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC mreže sa batch normalizacijom\n",
    "Sada kada ste implementirali batch normalizaciju, vratite se na `FullyConnectedNet`  u fajlu `funkcije/fc_net.py`. Izmijenite implementaciju da dodate batch normalizaciju.\n",
    "\n",
    "Konkretno, kada je `normalization` zastavica podešena na `\"batchnorm\"` u konstruktoru, trebate ubaciti batch normalizaciju ispred svake ReLU aktivacione funkcije. Izlazi iz poslednjeg sloja mreže ne treba da se noramlizuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# Treba očekivati greške u opsegu 1e-4~1e-10 za W, \n",
    "# odnosno 1e-08~1e-10 za b,\n",
    "# i 1e-08~1e-09 za beta i gammas.\n",
    "for reg in [0, 3.14]:\n",
    "    print('Provjera sa reg = ', reg)\n",
    "    model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            normalization='batchnorm')\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print('Početni loss: ', loss)\n",
    "\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print('%s relativna greška: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "    if reg == 0: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm za duboke mreže\n",
    "Pokrenuti sledeću ćeliju za treniranje šestoslojne mreže na uzorku od 1000 trening primjeraka sa i bez batch normalizacije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')\n",
    "model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True,print_every=20)\n",
    "bn_solver.train()\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokretanjem sledeće ćelije će se vizuelizovati rezultati dvije mreže iznad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(title, label, baseline, bn_solvers, plot_fn, bl_marker='.', bn_marker='.', labels=None):\n",
    "    \"\"\"pomoćna funkcija za crtanje trening istorije\"\"\"\n",
    "    plt.title(title)\n",
    "    plt.xlabel(label)\n",
    "    bn_plots = [plot_fn(bn_solver) for bn_solver in bn_solvers]\n",
    "    bl_plot = plot_fn(baseline)\n",
    "    num_bn = len(bn_plots)\n",
    "    for i in range(num_bn):\n",
    "        label='with_norm'\n",
    "        if labels is not None:\n",
    "            label += str(labels[i])\n",
    "        plt.plot(bn_plots[i], bn_marker, label=label)\n",
    "    label='baseline'\n",
    "    if labels is not None:\n",
    "        label += str(labels[0])\n",
    "    plt.plot(bl_plot, bl_marker, label=label)\n",
    "    plt.legend(loc='lower center', ncol=num_bn+1) \n",
    "\n",
    "    \n",
    "plt.subplot(3, 1, 1)\n",
    "plot_training_history('Trening loss','Iteracija', solver, [bn_solver], \\\n",
    "                      lambda x: x.loss_history, bl_marker='o', bn_marker='o')\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_training_history('Trening preciznost','Epoha', solver, [bn_solver], \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_training_history('Validaciona preciznost','Epoha', solver, [bn_solver], \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalizacija i inicijalizacija\n",
    "Sada ćemo napraviti mali eksperiment da proučimo interakciju između batch normalizacije i inicijalizacije težina.\n",
    "\n",
    "Prva ćelija će trenirati osmoslojnu mrežu sa i bez batch normalizacije koristeći različite inicijalizacije težina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "hidden_dims = [50, 50, 50, 50, 50, 50, 50]\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_solvers_ws = {}\n",
    "solvers_ws = {}\n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "    print('Weight scale %d / %d' % (i + 1, len(weight_scales)))\n",
    "    bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')\n",
    "    model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "\n",
    "    bn_solver = Solver(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "    bn_solver.train()\n",
    "    bn_solvers_ws[weight_scale] = bn_solver\n",
    "\n",
    "    solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "    solver.train()\n",
    "    solvers_ws[weight_scale] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crtanje rezultata eksperimenta\n",
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "    best_train_accs.append(max(solvers_ws[ws].train_acc_history))\n",
    "    bn_best_train_accs.append(max(bn_solvers_ws[ws].train_acc_history))\n",
    "\n",
    "    best_val_accs.append(max(solvers_ws[ws].val_acc_history))\n",
    "    bn_best_val_accs.append(max(bn_solvers_ws[ws].val_acc_history))\n",
    "\n",
    "    final_train_loss.append(np.mean(solvers_ws[ws].loss_history[-100:]))\n",
    "    bn_final_train_loss.append(np.mean(bn_solvers_ws[ws].loss_history[-100:]))\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Najbolja val preciznost vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Najbolja val preciznost')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-o', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Najbolja trening preciznost vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Najbolja trening preciznost')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Krajnji trening loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Krajnji trening loss')\n",
    "plt.semilogx(weight_scales, final_train_loss, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "plt.gca().set_ylim(1.0, 3.5)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalizacija i veličina batch-a\n",
    "Sada ćemo napraviti eksperiment o interakciji batch normalizacije i veličini batch-a.\n",
    "\n",
    "Prva ćelija će trenirati šestoslojnu mrežu sa i bez batch normalizacije koristeći različite veličine batch-a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batchsize_experiments(normalization_mode):\n",
    "    np.random.seed(231)\n",
    "    \n",
    "    hidden_dims = [100, 100, 100, 100, 100]\n",
    "    num_train = 1000\n",
    "    small_data = {\n",
    "      'X_train': data['X_train'][:num_train],\n",
    "      'y_train': data['y_train'][:num_train],\n",
    "      'X_val': data['X_val'],\n",
    "      'y_val': data['y_val'],\n",
    "    }\n",
    "    n_epochs=10\n",
    "    weight_scale = 2e-2\n",
    "    batch_sizes = [5,10,50]\n",
    "    lr = 10**(-3.5)\n",
    "    solver_bsize = batch_sizes[0]\n",
    "\n",
    "    print('Bez normalizacije: batch size = ',solver_bsize)\n",
    "    model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "    solver = Solver(model, small_data,\n",
    "                    num_epochs=n_epochs, batch_size=solver_bsize,\n",
    "                    update_rule='adam',\n",
    "                    optim_config={\n",
    "                      'learning_rate': lr,\n",
    "                    },\n",
    "                    verbose=False)\n",
    "    solver.train()\n",
    "    \n",
    "    bn_solvers = []\n",
    "    for i in range(len(batch_sizes)):\n",
    "        b_size=batch_sizes[i]\n",
    "        print('Normalizacija: batch size = ',b_size)\n",
    "        bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=normalization_mode)\n",
    "        bn_solver = Solver(bn_model, small_data,\n",
    "                        num_epochs=n_epochs, batch_size=b_size,\n",
    "                        update_rule='adam',\n",
    "                        optim_config={\n",
    "                          'learning_rate': lr,\n",
    "                        },\n",
    "                        verbose=False)\n",
    "        bn_solver.train()\n",
    "        bn_solvers.append(bn_solver)\n",
    "        \n",
    "    return bn_solvers, solver, batch_sizes\n",
    "\n",
    "batch_sizes = [5,10,50]\n",
    "bn_solvers_bsize, solver_bsize, batch_sizes = run_batchsize_experiments('batchnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plot_training_history('Trening preciznost (Batch Normalization)','Epoha', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_training_history('Validaciona preciznost (Batch Normalization)','Epoha', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalizacija\n",
    "Batch normalizacija je efikasna u olakšavanju treniranja mreža, ali zavisnost od veličine batch-a je čini manje korisnom u kompleksnim mrežama koje imaju ograničenja u vidu hardvera itd. \n",
    "\n",
    "Nekoliko alternativa je predloženo da bi se ovaj problem izbjegao. Jedna od takvih tehnika je Layer normalizacija [4]. Umjesto normalizacije po batch-u, normalizacija se vrši po karakteristikama.\n",
    "\n",
    "[4] [Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer Normalization.\" stat 1050 (2016): 21.](https://arxiv.org/pdf/1607.06450.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalizacija: Implementacija\n",
    "\n",
    "Sada ćete implementirati layer normalizaciju. Ovdje ne bi trebalo da bude većih poteškoća s obzirom da je konceptualno slična batch normalizaciji.\n",
    "\n",
    "Treba odraditi:\n",
    "\n",
    "* U `funkcije/layers.py`, implementirati prolaz unaprijed za layer normalizaciju u funkciji `layernorm_forward`. \n",
    "\n",
    "* U `funkcije/layers.py`, implementirati prolaz unazad za layer normalizaciju u funkciji `layernorm_backward`. \n",
    "\n",
    "* Modifikovati `funkcije/fc_net.py` da bi se dodala layer normalizacija `FullyConnectedNet`. Kada `normalization` zastavica je podešena na `\"layernorm\"` u konstruktoru, trebate ubaciti layer normalizaciju prije svake ReLU nelinearnosti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D1, D2, D3 =4, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Prije layer normalizacije:')\n",
    "print_mean_std(a,axis=1)\n",
    "\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "# Srednje vrijednosti bi trebalo da budu približno nula, a standardne devijacije približno jedinici\n",
    "print('Posle layer normalizacije (gamma=1, beta=0)')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)\n",
    "\n",
    "gamma = np.asarray([3.0,3.0,3.0])\n",
    "beta = np.asarray([5.0,5.0,5.0])\n",
    "# Sada srednje vrijednosti bi trebalo da budu približno beta, a standardne devijacije približno gamma\n",
    "print('Posle layer normalizacije (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "ln_param = {}\n",
    "fx = lambda x: layernorm_forward(x, gamma, beta, ln_param)[0]\n",
    "fg = lambda a: layernorm_forward(x, a, beta, ln_param)[0]\n",
    "fb = lambda b: layernorm_forward(x, gamma, b, ln_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = layernorm_forward(x, gamma, beta, ln_param)\n",
    "dx, dgamma, dbeta = layernorm_backward(dout, cache)\n",
    "\n",
    "#Trebate očekivati relativne greške između 1e-12 i 1e-8\n",
    "print('dx greška: ', rel_error(dx_num, dx))\n",
    "print('dgamma greška: ', rel_error(da_num, dgamma))\n",
    "print('dbeta greška: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer normalizacija i veličina batch-a\n",
    "\n",
    "Sada ćemo pokrenuti prethodni eksperiment sa veličinom batch-a sa layer normalizacijom umjesto batch normalizacije. Treba primijetiti da je manji uticaj veličine batch-a na istoriju treniranja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_solvers_bsize, solver_bsize, batch_sizes = run_batchsize_experiments('layernorm')\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_training_history('Trening preciznost (Layer Normalization)','Epoha', solver_bsize, ln_solvers_bsize, \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_training_history('Validaciona preciznost (Layer Normalization)','Epoha', solver_bsize, ln_solvers_bsize, \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
